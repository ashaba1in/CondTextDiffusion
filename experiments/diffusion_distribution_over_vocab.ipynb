{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68e20eca-c69e-4e73-82a8-7e2072fbb91e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99885c07-7986-472e-af03-a4b9e66f2617",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import argparse\n",
    "import torch.distributed as dist\n",
    "import ml_collections\n",
    "from datasets import disable_progress_bar\n",
    "from transformers import BertConfig\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"/home/vmeshchaninov/DiffusionTextGeneration-cond-ca/\")\n",
    "\n",
    "from diffusion_holder import DiffusionRunner\n",
    "from utils.util import set_seed, _BERT_SMALL, dict_to_cuda\n",
    "from estimation_utils.util import estimate_model, reduce_metrics, gather_texts\n",
    "import diffusion_utils.schedulers as schedulers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4fd508cb-3c06-413f-b031-266abea5de2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_config():\n",
    "    config = ml_collections.ConfigDict()\n",
    "\n",
    "    training = config.training = ml_collections.ConfigDict()\n",
    "    training.ode_sampling = False\n",
    "    training.checkpoints_folder = '../checkpoints'\n",
    "    training.batch_size = 512\n",
    "    config.checkpoints_prefix = None\n",
    "\n",
    "    validation = config.validation = ml_collections.ConfigDict()\n",
    "    validation.batch_size = 512\n",
    "\n",
    "    sde = config.sde = ml_collections.ConfigDict()\n",
    "    sde.typename = 'vp-sde'\n",
    "    sde.solver = 'euler'\n",
    "    sde.N = 200\n",
    "    sde.beta_min = 0.1\n",
    "    sde.beta_max = 20\n",
    "    sde.ode_sampling = False\n",
    "    sde.scheduler = schedulers.CosineSD(d=10)\n",
    "\n",
    "    model = config.model = ml_collections.ConfigDict()\n",
    "    model.ema_rate = 0.9999\n",
    "    model.enc_type = \"base\"\n",
    "    model.embeddings_type = \"encodings\"\n",
    "    model.dif_enc_type = \"base\"\n",
    "    model.downstream_task = \"sst2\"  # \"qqp\"\n",
    "    model.dataset = \"wikipedia\"  # \"glue\"\n",
    "    model.prediction = \"x_0\"\n",
    "    model.loss = \"L_x_0\"\n",
    "    model.decoder_path = \"decoder-wikipedia-128.pth\"  # \"decoder-wikipedia-128.pth\"  # \"decoder-t5_base-wikipedia-128.pth\"\n",
    "\n",
    "    data = config.data = ml_collections.ConfigDict()\n",
    "    data.max_sequence_len = 96\n",
    "    data.enc_bert_mean = \"/home/vmeshchaninov/DiffusionTextGeneration-cond-ca/data/encodings-bert_base-wiki-mean.pt\"\n",
    "    data.enc_bert_std = \"/home/vmeshchaninov/DiffusionTextGeneration-cond-ca/data/encodings-bert_base-wiki-std.pt\"\n",
    "    data.enc_t5_mean = \"/home/vmeshchaninov/DiffusionTextGeneration-cond-ca/data/encodings-t5-wiki-mean.pth\"\n",
    "    data.enc_t5_std = \"/home/vmeshchaninov/DiffusionTextGeneration-cond-ca/data/encodings-t5-wiki-std.pth\"\n",
    "\n",
    "    config.finetuning = False\n",
    "    config.seed = 0\n",
    "    config.ddp = False\n",
    "    config.bert_config = BertConfig.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "    config.project_name = \"bert-conditional-exps\"\n",
    "\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec95c7e3-c9a7-451c-b73e-6539fcb59869",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at t5-base were not used when initializing T5EncoderModel: ['decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.final_layer_norm.weight']\n",
      "- This IS expected if you are initializing T5EncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5EncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of T5EncoderModel were not initialized from the model checkpoint at t5-base and are newly initialized: ['enc_normalizer.enc_std', 'enc_normalizer.enc_mean']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertEncoderModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertEncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertEncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertEncoderModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['enc_normalizer.enc_std', 'enc_normalizer.enc_mean']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "config = create_config()\n",
    "config.checkpoints_prefix = \"wikipedia-sst2-prediction=x_0-loss=L_x_0-enc=base-bert=base-kl_cf=0.0-seq_len=96-clipgrad=1.0-lr=0.0002-min_lr=0.0002-lin_input=True-seed=0-wd=0.01-batch=512-t5-bert-womask_1000000_\"\n",
    "\n",
    "diffusion = DiffusionRunner(config, latent_mode=config.model.embeddings_type, eval=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "125d0f36-cbbf-4ab7-8ead-eca23a036c1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "diffusion.config.validation.bnnatch_size = batch_size\n",
    "\n",
    "diffusion.set_valid_data_generator()\n",
    "loader = iter(diffusion.valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8364643f-3b00-4ab4-a2ce-1c635e34c06e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "condition = next(loader)\n",
    "cond = dict_to_cuda({\"cond\": condition[\"cond_ids\"], \"cond_mask\": condition[\"cond_mask\"]},)\n",
    "cond_X = diffusion.encoder_cond(**{\"input_ids\": cond[\"cond\"], \"attention_mask\": cond[\"cond_mask\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "19a07080-9f3b-4ed9-8c2f-d9b70f1c9d1c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:19<00:00, 10.30it/s]\n"
     ]
    }
   ],
   "source": [
    "pred_embeddings = diffusion.pred_embeddings(batch_size, cond_X=cond_X, cond_mask=cond[\"cond_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ac66c0c1-72f4-4296-ac6b-9cdd7bf08e20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred_embeddings = diffusion.gen_enc_normalizer.denormalize(pred_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5a9996c5-61cb-4422-b709-0f1c813d0423",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logits = diffusion.decoder(pred_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c0271a7d-23a3-407c-8778-722fe9411bd3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "probs = torch.softmax(logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6a83fd48-efe3-429a-bd92-81d838d731ce",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probabilities:                indexes:                      \n",
      "[100, 0, 0]                   [101, 1037, 1043]             [CLS],  a,  g,                \n",
      "[100, 0, 0]                   [1001, 1008, 1526]            #,  *,  †,                    \n",
      "[100, 0, 0]                   [1001, 1008, 1030]            #,  *,  @,                    \n",
      "[99, 0, 0]                    [1045, 1051, 9932]            i,  o,  ai,                   \n",
      "[91, 0, 0]                    [3211, 3089, 5638]            ##ki,  ##ri,  ##bi,           \n",
      "[32, 3, 3]                    [3067, 13173, 23214]          mine,  kara,  meiji,          \n",
      "[100, 0, 0]                   [1000, 1006, 1524]            \",  (,  ”,                    \n",
      "[99, 0, 0]                    [1012, 102, 1037]             .,  [SEP],  a,                \n",
      "[71, 18, 2]                   [10556, 5003, 7842]           ka,  ma,  sa,                 \n",
      "[14, 13, 12]                  [28260, 23778, 16566]         ##kura,  ##rika,  ##kara,     \n",
      "[67, 3, 3]                    [3089, 11077, 16069]          ##ri,  ##hara,  ##mura,       \n",
      "[99, 0, 0]                    [1005, 1055, 2011]            ',  s,  by,                   \n",
      "[100, 0, 0]                   [1055, 1005, 2128]            s,  ',  re,                   \n",
      "[96, 3, 0]                    [2034, 2834, 3083]            first,  debut,  1st,          \n",
      "[99, 0, 0]                    [2309, 3895, 2028]            single,  singles,  one,       \n",
      "[100, 0, 0]                   [1010, 1025, 1007]            ,,  ;,  ),                    \n",
      "[92, 2, 1]                    [2019, 2996, 2309]            an,  studio,  single,         \n",
      "[99, 0, 0]                    [2201, 8775, 7654]            album,  assignment,  acquisition,  \n",
      "[100, 0, 0]                   [1000, 1006, 1037]            \",  (,  a,                    \n",
      "[28, 22, 5]                   [9805, 9152, 11333]           yu,  ni,  wa,                 \n",
      "[49, 12, 4]                   [3211, 3089, 5283]            ##ki,  ##ri,  ##ku,           \n",
      "[33, 19, 9]                   [2053, 11333, 6583]           no,  wa,  na,                 \n",
      "[22, 12, 11]                  [11937, 9805, 19808]          ta,  yu,  mina,               \n",
      "[86, 5, 1]                    [3211, 3089, 11151]           ##ki,  ##ri,  ##kai,          \n",
      "[100, 0, 0]                   [1000, 1006, 2078]            \",  (,  ##n,                  \n",
      "[99, 0, 0]                    [2207, 8287, 2405]            released,  releasing,  published,  \n",
      "[100, 0, 0]                   [2011, 3501, 2026]            by,  ##j,  my,                \n",
      "[10, 7, 4]                    [19808, 22827, 9805]          mina,  kan,  yu,              \n",
      "[9, 9, 6]                     [27543, 17322, 18410]         ##bane,  ##iche,  ##iba,      \n",
      "[99, 0, 0]                    [2636, 5633, 2501]            records,  recordings,  record,  \n",
      "[99, 0, 0]                    [1012, 102, 1007]             .,  [SEP],  ),                \n",
      "[99, 0, 0]                    [1996, 1037, 1011]            the,  a,  -,                  \n",
      "[99, 0, 0]                    [2309, 3895, 2028]            single,  singles,  one,       \n",
      "[98, 0, 0]                    [2950, 2421, 3774]            includes,  include,  consists,  \n",
      "[59, 12, 11]                  [3104, 9573, 2047]            cover,  boot,  new,           \n",
      "[45, 40, 3]                   [4472, 4617, 17241]           covers,  versions,  adaptations,  \n",
      "[99, 0, 0]                    [1997, 2011, 1037]            of,  by,  a,                  \n",
      "[55, 33, 2]                   [10556, 5003, 7842]           ka,  ma,  sa,                 \n",
      "[9, 8, 8]                     [5283, 15750, 29312]          ##ku,  ##uka,  ##miya,        \n",
      "[52, 7, 4]                    [16069, 3089, 20224]          ##mura,  ##ri,  ##kari,       \n",
      "[99, 0, 0]                    [1005, 1055, 2474]            ',  s,  la,                   \n",
      "[100, 0, 0]                   [1055, 1005, 2128]            s,  ',  re,                   \n",
      "[99, 0, 0]                    [2034, 2834, 3083]            first,  debut,  1st,          \n",
      "[99, 0, 0]                    [2996, 2019, 3948]            studio,  an,  solo,           \n",
      "[99, 0, 0]                    [2201, 3947, 2479]            album,  effort,  island,      \n",
      "[100, 0, 0]                   [1010, 1517, 1025]            ,,  —,  ;,                    \n",
      "[100, 0, 0]                   [1000, 2011, 2243]            \",  by,  ##k,                 \n",
      "[73, 6, 1]                    [11333, 9805, 19808]          wa,  yu,  mina,               \n",
      "[86, 7, 0]                    [3211, 3089, 5283]            ##ki,  ##ri,  ##ku,           \n",
      "[19, 10, 9]                   [13173, 11333, 19808]         kara,  wa,  mina,             \n",
      "[9, 7, 5]                     [28414, 3211, 13469]          ##rito,  ##ki,  ##vie,        \n",
      "[85, 8, 0]                    [3211, 3089, 4674]            ##ki,  ##ri,  ##tte,          \n",
      "[100, 0, 0]                   [1000, 1006, 2299]            \",  (,  song,                 \n",
      "[100, 0, 0]                   [1998, 1007, 2092]            and,  ),  well,               \n",
      "[100, 0, 0]                   [1996, 1005, 2011]            the,  ',  by,                 \n",
      "[81, 6, 2]                    [2599, 3098, 2882]            lead,  opening,  grand,       \n",
      "[99, 0, 0]                    [2309, 3895, 2028]            single,  singles,  one,       \n",
      "[100, 0, 0]                   [1000, 2011, 2299]            \",  by,  song,                \n",
      "[35, 15, 4]                   [9805, 11333, 13173]          yu,  wa,  kara,               \n",
      "[83, 11, 1]                   [3089, 3211, 5638]            ##ri,  ##ki,  ##bi,           \n",
      "[28, 6, 4]                    [9152, 11937, 9805]           ni,  ta,  yu,                 \n",
      "[81, 3, 2]                    [3211, 3089, 11151]           ##ki,  ##ri,  ##kai,          \n",
      "[99, 0, 0]                    [1000, 1524, 2299]            \",  ”,  song,                 \n",
      "[99, 0, 0]                    [102, 1007, 15185]            [SEP],  ),  ##rb,             \n",
      "[100, 0, 0]                   [0, 1516, 1084]               [PAD],  –,  ´,                \n",
      "[99, 0, 0]                    [0, 1516, 1517]               [PAD],  –,  —,                \n",
      "[99, 0, 0]                    [0, 1516, 1517]               [PAD],  –,  —,                \n",
      "[99, 0, 0]                    [0, 1516, 1517]               [PAD],  –,  —,                \n",
      "[100, 0, 0]                   [0, 1516, 1084]               [PAD],  –,  ´,                \n",
      "[99, 0, 0]                    [0, 1516, 21932]              [PAD],  –,  ##´s,             \n",
      "[100, 0, 0]                   [0, 1516, 1084]               [PAD],  –,  ´,                \n",
      "[100, 0, 0]                   [0, 1516, 1084]               [PAD],  –,  ´,                \n",
      "[99, 0, 0]                    [0, 1517, 1516]               [PAD],  —,  –,                \n",
      "[99, 0, 0]                    [0, 1523, 1517]               [PAD],  “,  —,                \n",
      "[99, 0, 0]                    [0, 1529, 1523]               [PAD],  …,  “,                \n",
      "[99, 0, 0]                    [0, 1529, 1084]               [PAD],  …,  ´,                \n",
      "[99, 0, 0]                    [0, 1529, 1516]               [PAD],  …,  –,                \n",
      "[99, 0, 0]                    [0, 1529, 1516]               [PAD],  …,  –,                \n",
      "[99, 0, 0]                    [0, 1529, 1516]               [PAD],  …,  –,                \n",
      "[99, 0, 0]                    [0, 1084, 1516]               [PAD],  ´,  –,                \n",
      "[99, 0, 0]                    [0, 1529, 1516]               [PAD],  …,  –,                \n",
      "[99, 0, 0]                    [0, 1516, 1517]               [PAD],  –,  —,                \n",
      "[99, 0, 0]                    [0, 1516, 1517]               [PAD],  –,  —,                \n",
      "[99, 0, 0]                    [0, 1517, 1084]               [PAD],  —,  ´,                \n",
      "[99, 0, 0]                    [0, 1517, 1084]               [PAD],  —,  ´,                \n",
      "[100, 0, 0]                   [0, 1516, 1517]               [PAD],  –,  —,                \n",
      "[99, 0, 0]                    [0, 1516, 1517]               [PAD],  –,  —,                \n",
      "[100, 0, 0]                   [0, 1516, 1084]               [PAD],  –,  ´,                \n",
      "[99, 0, 0]                    [0, 1516, 1517]               [PAD],  –,  —,                \n",
      "[99, 0, 0]                    [0, 1516, 1523]               [PAD],  –,  “,                \n",
      "[99, 0, 0]                    [0, 1516, 1991]               [PAD],  –,  ．,                \n",
      "[99, 0, 0]                    [0, 1516, 1517]               [PAD],  –,  —,                \n",
      "[99, 0, 0]                    [0, 1516, 1517]               [PAD],  –,  —,                \n",
      "[99, 0, 0]                    [0, 1084, 1517]               [PAD],  ´,  —,                \n",
      "[99, 0, 0]                    [0, 1084, 29658]              [PAD],  ´,  ##´,              \n",
      "[99, 0, 0]                    [0, 1516, 1084]               [PAD],  –,  ´,                \n"
     ]
    }
   ],
   "source": [
    "n = 9\n",
    "k = 3\n",
    "\n",
    "p_s = (torch.topk(probs, k=k, dim=-1)[0][n] * 100).int()\n",
    "ind_s = (torch.topk(probs, k=k, dim=-1)[1][n]).int()\n",
    "\n",
    "n_chars_per_col = 30\n",
    "sep = ' ' \n",
    "\n",
    "print(f\"probabilities:{sep * (n_chars_per_col - 14)}indexes:{sep * (n_chars_per_col - 8)}\")\n",
    "for i in range(96):\n",
    "    col1 = f\"{p_s[i].tolist()}\"\n",
    "    col2 = f\"{ind_s[i].tolist()}\"\n",
    "\n",
    "    col3 = \"\"\n",
    "    for ind in ind_s[i].tolist():\n",
    "        token = diffusion.tokenizer_gen.decode(ind)\n",
    "        col3 += f\"{token},  \"    \n",
    "    \n",
    "    print(f\"{col1}{sep * (n_chars_per_col - len(col1))}\" \\\n",
    "          f\"{col2}{sep * (n_chars_per_col - len(col2))}\" \\\n",
    "          f\"{col3}{sep * (n_chars_per_col - len(col3))}\"\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ba40a785-d261-4261-974c-2087b2d9b3aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 96, 30522])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diffusion.gen_tokenizer.decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5acc34-625f-46ef-a7d2-588d6f920643",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [.conda-fap2_env]",
   "language": "python",
   "name": "conda-env-.conda-fap2_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
